# A conda environment for vLLM.
RUN {{cache_mount_str}} <<EOF
set -euxo pipefail

# keep the package list sorted
micromamba install -c conda-forge\
 cattrs\
 loguru\
 numpy\<2.0.0\
 nvtx\
 pandas\
 psycopg2\
 python=3.10.14\
 scikit-image\
 scipy\
 tabulate\

# only for packages not available in conda
# qwen needs a specific version of vllm and transformers
pip install --no-cache-dir\
 accelerate==1.3.0\
 azure-identity>=1.0.0\
 azure-storage-blob>=12.0.0\
 easydict\
 ngcsdk==3.64.4\
 opencv-python-headless\
 pulp\
 pynvml==11.5.3\
 pytest\
 qwen-vl-utils==0.0.10\
 sqlalchemy\
 tenacity\
 transformers==4.51.3\
 vllm==0.8.5.post1\
 webdataset\

# tenacity needed by tests.cosmos_curate.scripts.test_launch_slurm
# easydict needed by cosmos_curate.models.internvideo2_mm
# pulp needed by tests.cosmos_curate.pipelines.video.embedding.test_internvideo2_embedding,
#    which in turn requires cosmos_xenna for run_pipeline
# sqlalchemy, psycopg2 needed by cosmos_curate.pipelines.av.utils.postgres_schema

# vllm has ray[cgraph]>2.43.0 dependency for pipeline parallelism which is not used in cosmos-curate currrently
# For now ensure ray version is 2.40.0 for compatibility
pip install ray[default]==2.46.0

# cleanup
micromamba clean --all --yes
rm -rf /root/.cache/pip
EOF
