workflow:
  rules:
    # Run pipeline for merge requests
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    # Run pipeline for merged commits to default branch
    - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH'
    # Run pipeline for scheduled pipelines (cron)
    - if: '$CI_PIPELINE_SOURCE == "schedule"'
    # Run pipeline for E2E testing
    - if: $TEST_END_TO_END == "True"
    # Run manual pipeline for web-triggered runs
    - if: '$CI_PIPELINE_SOURCE == "web"'
    # Skip everything else
    - when: never
  auto_cancel:
    on_new_commit: interruptible

default:
  tags:
    - vm-builder  # Default to shell executor
  before_script:
    - set -x
  interruptible: true

variables:
  # Versions
  PYTHON_VERSION: "3.10.18"
  RUFF_VERSION: "0.11.8"

  # Git config
  GIT_STRATEGY: fetch
  GIT_DEPTH: 1
  GIT_SUBMODULE_STRATEGY: normal
  GIT_SUBMODULE_DEPTH: 1

  # GitLab container registry
  CURATOR_IMAGE: ${CI_REGISTRY_IMAGE}/curator

  #Pytest configuration
  PYTEST_XDIST_WORKERS: 4

  # NVCF configuration
  NVCF_ORG_ID: "$NVCF_ORG_ID"
  NVCF_DEV_BASE_IMAGE: "dev-cosmos-curator"  # Base name for NVCF dev images
  NVCF_DEV_IMAGE: "nvcr.io/${NVCF_ORG_ID}/${NVCF_DEV_BASE_IMAGE}"  # Complete NVCF dev image path
  NVCF_STAGING_BASE_IMAGE: "staging-cosmos-curator"  # Base name for NVCF staging images
  NVCF_STAGING_IMAGE: "nvcr.io/${NVCF_ORG_ID}/${NVCF_STAGING_BASE_IMAGE}"  # Complete NVCF staging image path
  NVCF_PROD_BASE_IMAGE: "prod-cosmos-curator"  # Base name for NVCF prod images
  NVCF_PROD_IMAGE: "nvcr.io/${NVCF_ORG_ID}/${NVCF_PROD_BASE_IMAGE}"  # Complete NVCF prod image path
  NGC_NVCF_ORG: $NVCF_ORG_ID  # Needed for helm deploy json template
  NVCF_MAX_CONCURRENCY: 2
  NVCF_INSTANCE_COUNT: 1

  # AWS configuration
  AWS_ACCESS_KEY_ID: "$AWS_ACCESS_KEY_ID"
  AWS_SECRET_ACCESS_KEY: "$AWS_SECRET_ACCESS_KEY"
  AWS_DEFAULT_REGION: "$AWS_DEFAULT_REGION"
  S3_INPUT_VIDEO_PATH: "s3://${AWS_S3_BUCKET_NAME}/cicd-curator-oss/raw_videos/samples"
  S3_OUTPUT_PATH: "s3://${AWS_S3_BUCKET_NAME}/cicd-curator-oss/output/${CI_PIPELINE_ID}"
  S3_OUTPUT_CLIP_PATH: "${S3_OUTPUT_PATH}/cosmos-curator-nvcf-helm/raw_clips"
  S3_OUTPUT_DEDUP_PATH: "${S3_OUTPUT_PATH}/cosmos-curator-nvcf-helm/dedup_results"
  S3_OUTPUT_DATASET_PATH: "${S3_OUTPUT_PATH}/cosmos-curator-nvcf-helm/datasets"

  # HELM configuration
  HELM_THANOS_RECEIVER_URL: "$HELM_THANOS_RECEIVER_URL"
  HELM_BYO_METRICS_RECEIVER_CLIENT_CRT: $HELM_BYO_METRICS_RECEIVER_CLIENT_CRT
  HELM_BYO_METRICS_RECEIVER_CLIENT_KEY: $HELM_BYO_METRICS_RECEIVER_CLIENT_KEY
  HELM_GPU_REQUESTS: 1
  HELM_GPU_LIMITS: 1
  HELM_SHMEM_LIMIT: 500Gi
  HELM_IMAGE_TAG:
  TEST_END_TO_END:
    description: "Run cosmos-curator end-to-end test pipeline"
    value: "False"
    options:
      - "True"
      - "False"
  HELM_DEBUG_KEEP_FAILED_DEPLOYMENTS:
    description: "On failure, do not delete the deployment"
    value: "False"
    options:
      - "True"
      - "False"
  CICD_HELM_CHART_NAME:
    description: "Helm chart name for the CICD pipeline"
    value: "cosmos-curate"
  CICD_HELM_CHART_VERSION:
    description: "Helm chart version for the CICD pipeline"
    value: "2.0.5"

  # Manual NVCF testing parameters
  NVCF_GPU_TYPE:
    description: "GPU type for NVCF testing (e.g., L40S, H100)"
    value: "L40S"
  NVCF_INSTANCE_TYPE:
    description: "Instance type for NVCF testing (e.g., OCI.GPU.L40S_1x, OCI.GPU.H100_1x)"
    value: "OCI.GPU.L40S_1x"
  NVCF_CUSTOM_BACKEND:
    description: "Custom NVCF backend for testing (overrides default GitLab CI/CD variable)"
    value: ""

stages:
  - lint
  - test
  - build
  - end-to-end
  - post-merge

# Reusable snippets (anchors)
.snippets:
  # Rules for pre-merge jobs
  pre_merge_rules: &pre_merge_rules
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" || $TEST_END_TO_END == "True"'
    - when: never

  # Rules for post-merge jobs
  post_merge_rules: &post_merge_rules
    - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH'
    - when: never

  # Rules for merge result pipelines only
  merge_result_rules: &merge_result_rules
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_EVENT_TYPE != "merge_train"'
    - when: never

  # Rules for merge result pipelines and web triggers
  merge_result_and_web_rules: &merge_result_and_web_rules
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_EVENT_TYPE != "merge_train"'
    - if: '$CI_PIPELINE_SOURCE == "web"'
    - when: never

  # Rules for jobs that should run both before and after merging
  pre_post_merge_rules: &pre_post_merge_rules
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" || $TEST_END_TO_END == "True"'
    - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH'
    - when: never

  # Rules for jobs that should run on pre/post merge AND web triggers
  pre_post_merge_and_web_rules: &pre_post_merge_and_web_rules
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" || $TEST_END_TO_END == "True"'
    - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH'
    - if: '$CI_PIPELINE_SOURCE == "web"'
    - when: never

  # Rules for scheduled (cron) jobs
  schedule_rules: &schedule_rules
    - if: '$CI_PIPELINE_SOURCE == "schedule"'
    - when: never

  # Configure Docker authentication for GitLab/NVCF registries
  docker_login: &docker_login |
    export DOCKER_CONFIG="${CI_PROJECT_DIR}/.docker"
    mkdir -p "${DOCKER_CONFIG}"
    docker login -u gitlab-ci-token -p ${CI_JOB_TOKEN} "${CI_REGISTRY}"
    echo "$NVCF_KEY" | docker login nvcr.io --username '$oauthtoken' --password-stdin

  # Set up micromamba environment for Python dependencies
  setup_micromamba: &setup_micromamba |
    # We now use locally installed micromamba
    # Shell runner: /usr/local/bin/micromamba
    # Docker runner: /usr/bin/micromamba (if using the micromamba Docker image)
    # Configure micromamba environment
    export MAMBA_ROOT_PREFIX="$(pwd)/micromamba"
    eval "$(micromamba shell hook -s posix)"
    # Create Python environment for curator
    micromamba create -n curator python=${PYTHON_VERSION} poetry -y
    micromamba activate curator

  # Activate an existing micromamba environment
  activate_micromamba: &activate_micromamba |
    # Configure micromamba environment
    export MAMBA_ROOT_PREFIX="$(pwd)/micromamba"
    eval "$(micromamba shell hook -s posix)"
    # Create Python environment for curator
    micromamba activate curator

  # Clean up micromamba environment
  teardown_micromamba: &teardown_micromamba |
    micromamba deactivate || true
    micromamba env remove -n curator -y || true

  # Install curator full requirements
  setup_curator: &setup_curator |
    poetry install --extras=local --no-interaction

  # Install curator CLI requirements
  setup_curator_cli: &setup_curator_cli |
    poetry install --no-interaction

ruff:
  stage: lint
  image: ghcr.io/astral-sh/ruff:${RUFF_VERSION}-alpine
  tags:
    - build-runner  # Override to Docker executor
  variables:
    GIT_SUBMODULE_STRATEGY: none
  rules: *pre_merge_rules
  script:
    - ruff format --check
    - ruff check

mypy:
  stage: lint
  image: mambaorg/micromamba:latest
  tags:
    - build-runner  # Override to Docker executor
  rules: *pre_merge_rules
  script:
    - *setup_micromamba
    - *setup_curator
    - mypy --pretty
    - *teardown_micromamba

unit_tests:
  stage: test
  needs: []
  variables:
    PYTEST_ADDOPTS: >-
      --junitxml=report.xml
      --cov=cosmos_curate
      --cov-report=term
      --cov-report=xml:unit-coverage.xml
      --cov-report=html:unit-htmlcov
  rules: *pre_post_merge_rules
  script:
    - *setup_micromamba
    - *setup_curator
    - pytest
    # Save the raw coverage data file
    - cp .coverage .coverage.unit_tests || true
    - *teardown_micromamba
  artifacts:
    when: always
    reports:
      junit: report.xml
    paths:
      - unit-htmlcov/
      - unit-coverage.xml
      - .coverage.unit_tests
    expire_in: 1 week

build_deploy_curator:
  stage: build
  rules: *pre_post_merge_and_web_rules
  script:
    # For web triggers, this is a no-op since we use existing images
    - |
      if [[ "$CI_PIPELINE_SOURCE" == "web" ]]; then
        echo "Web trigger detected - skipping image build, using existing images"
        exit 0
      fi
    # Set up environment
    - *docker_login
    - *setup_micromamba
    - *setup_curator_cli
    - IMAGE_TAG="${CI_COMMIT_TIMESTAMP%%T*}_${CI_COMMIT_SHORT_SHA}"
    - |
      cosmos-curate image build \
      --curator-path . \
      --image-name ${CURATOR_IMAGE} \
      --image-tag ${IMAGE_TAG}
    - FULL_IMAGE=${CURATOR_IMAGE}:${IMAGE_TAG}
    - echo "Built image ${FULL_IMAGE} from curator commit [${CI_COMMIT_SHA}]"
    - docker push ${FULL_IMAGE}
    - echo "Pushed image ${FULL_IMAGE} to GitLab registry"
    - NVCF_IMAGE=${NVCF_DEV_IMAGE}:${IMAGE_TAG}
    - docker buildx imagetools create -t ${NVCF_IMAGE} ${FULL_IMAGE}
    - echo "Copied to image ${NVCF_IMAGE} from curator commit [${CI_COMMIT_SHA}]"
    # Cleanup
    - *teardown_micromamba
  retry:
    max: 2
    when: always

# Run GPU tests on SLURM cluster
gpu_tests:
  stage: end-to-end
  rules: *pre_post_merge_rules
  needs: [ "build_deploy_curator" ]
  tags:
    - ${SLURM_RUNNER_TAG}
  variables:
    GIT_STRATEGY: none
    CI_BUILDS_DIR: ${NEMO_CI_PATH}/.config/${CI_JOB_ID}
    ENROOT_CONFIG_PATH: "${CI_BUILDS_DIR}/.enroot"
  script:
    # Set up Docker authentication
    - mkdir -p "${ENROOT_CONFIG_PATH}"
    - echo "machine ${CI_REGISTRY/:5005/} login gitlab-ci-token password ${CI_JOB_TOKEN}" > $ENROOT_CONFIG_PATH/.credentials

    # Pull the specific image version to test
    - IMAGE_TAG="${CI_COMMIT_TIMESTAMP%%T*}_${CI_COMMIT_SHORT_SHA}"
    - FULL_IMAGE=${CURATOR_IMAGE}:${IMAGE_TAG}
    - BUILD_IMAGE_NAME_SBATCH="${FULL_IMAGE/:5005\///}"
    # Submit SLURM job using the Docker image
    - DATA_DIR=/lustre/fsw/coreai_dlalgo_ci/datasets/nemo_curator/video
    - MODEL_DIR=/lustre/fsw/coreai_dlalgo_ci/nemo_video_curator/models
    - AWS_CREDS_PATH="/lustre/fsw/coreai_dlalgo_ci/datasets/nemo_curator/video/awscreds"
    - MOUNTS="${DATA_DIR}:/config/data,${MODEL_DIR}:/config/models,${AWS_CREDS_PATH}:/dev/shm/s3_creds_file,${CI_PROJECT_DIR}:/config/project"
    - NODES=1
    - export LOCAL_DOCKER_ENV_VAR_NAME=1
    # Create bash script for running environment tests
    - |
      cat > run_env_tests.sh << 'EOL'
      #!/bin/bash
      set -e

      # Run tests for each environment with unique report files and coverage
      for env in default video-splitting unified; do
        echo "Running tests for $env environment"
        pixi run -e $env pytest -m env -n ${PYTEST_XDIST_WORKERS} \
          --junitxml="/config/project/$env-report.xml" \
          --cov=cosmos_curate \
          --cov-report=term \
          --cov-report=xml:/config/project/$env-coverage.xml \
          --cov-report=html:/config/project/$env-htmlcov \
          tests/cosmos_curate/pipelines
        
        # Save the coverage data file for each environment
        if [ -f .coverage ]; then
          cp .coverage /config/project/.coverage.gpu_$env
        fi
      done
      EOL
    - chmod +x run_env_tests.sh
    # Run pytest for each environment; sleep 15 min before retry
    - >
      srun -A ${SLURM_ACCOUNT} \
           -p ${SLURM_PARTITION} \
           -N 1 \
           --container-image ${BUILD_IMAGE_NAME_SBATCH} \
           --container-mounts ${MOUNTS} \
           -J ${SLURM_ACCOUNT}-cosmos_curator_test.${CI_JOB_ID} \
           -t "02:00:00" \
           /config/project/run_env_tests.sh \
      || (sleep 900 && false)
  retry:
    max: 2
    when: always
  artifacts:
    when: always
    reports:
      junit:
        - "*-report.xml"
    paths:
      - "*-coverage.xml"
      - "*-htmlcov/"
      - "*-report.xml"
      - ".coverage.gpu_*"
    expire_in: 1 week

# Combine coverage from unit tests and GPU tests
# This job runs after both unit_tests and gpu_tests complete, combining coverage data from:
# - unit_tests: .coverage.unit_tests
# - gpu_tests: .coverage.gpu_*
# This provides a complete picture of test coverage across all test suites and environments.
# The coverage badge and MR coverage stats will use data from this job.
combine_coverage:
  variables:
    GIT_SUBMODULE_STRATEGY: none
  stage: end-to-end
  needs: ["unit_tests", "gpu_tests"]
  rules: *pre_post_merge_rules
  image: python:3.10
  tags:
    - build-runner  # Override to Docker executor
  script:
    - pip install coverage
    # Combine coverage data from both jobs
    - echo "Combining coverage from unit_tests and gpu_tests..."
    - echo "Available coverage files:"
    - ls -la .coverage.* || true
    - coverage combine .coverage.unit_tests .coverage.gpu_*
    # Generate final combined reports
    - coverage report
    - coverage xml
    - coverage html
  coverage: '/^TOTAL.*\s+(\d+\.\d+%)/'
  artifacts:
    when: always
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml
    paths:
      - coverage.xml
      - htmlcov/
    expire_in: 1 week

# SAST jobs
include:
  - template: Jobs/SAST.gitlab-ci.yml

semgrep-sast:
  needs: []
  tags:
    - build-runner  # Override to Docker executor
  variables:
    GIT_SUBMODULE_STRATEGY: none
    SAST_EXCLUDED_PATHS: "cosmos-xenna/**"
  rules: *pre_merge_rules

# Run end-to-end tests on NVCF cluster
nvcf-helm:
  stage: end-to-end
  needs: [ "build_deploy_curator" ]
  rules: *merge_result_and_web_rules
  allow_failure: false
  image: mambaorg/micromamba:latest
  tags:
    - build-runner  # Override to Docker executor
  variables:
    HELM_FUNCTION_NAME: "cosmos-curator-${CI_JOB_NAME_SLUG}-${CI_PIPELINE_ID}"
  before_script:
    - *setup_micromamba
    - *setup_curator_cli
    - |
      if ! which envsubst; then
        micromamba install -y -c conda-forge gettext jq
      fi
    - export HELM_BYO_METRICS_RECEIVER_CLIENT_CRT=$(echo $HELM_BYO_METRICS_RECEIVER_CLIENT_CRT_BASE64 | base64 -d | awk '{printf "%s\\n", $0}')
    - export HELM_BYO_METRICS_RECEIVER_CLIENT_KEY=$(echo $HELM_BYO_METRICS_RECEIVER_CLIENT_KEY_BASE64 | base64 -d | awk '{printf "%s\\n", $0}')
  script:
      - cosmos-curate nvcf config set --org $NVCF_ORG_ID --key $NVCF_KEY
      - export NVCF_BACKEND="${NVCF_CUSTOM_BACKEND:-$NVCF_BACKEND}"
      - |
        if [[ "$TEST_END_TO_END" == "True" ]]; then
          # this can be overridden by passing the HELM_IMAGE_TAG var in cicd variables
          export HELM_IMAGE_TAG=${HELM_IMAGE_TAG:-$(cosmos-curate nvcf image list-image-detail --iname $NVCF_DEV_BASE_IMAGE |grep latestTag |sed "s/['│,]//g" |awk '{print $2}')}
        elif [[ "$CI_PIPELINE_SOURCE" == "web" ]]; then
          # Web triggers use staging images
          export HELM_IMAGE_TAG=${HELM_IMAGE_TAG:-$(cosmos-curate nvcf image list-image-detail --iname $NVCF_STAGING_BASE_IMAGE |grep latestTag |sed "s/['│,]//g" |awk '{print $2}')}
        else
          export HELM_IMAGE_TAG="${CI_COMMIT_TIMESTAMP%%T*}_${CI_COMMIT_SHORT_SHA}"
        fi
      - |
        if [[ "$CI_PIPELINE_SOURCE" == "web" ]]; then
          NVCF_IMAGE=${NVCF_STAGING_IMAGE}:$HELM_IMAGE_TAG
        else
          NVCF_IMAGE=${NVCF_DEV_IMAGE}:$HELM_IMAGE_TAG
        fi
      # NOTE: create_helm_file.json will now contain keys/certificates so do not print its contents
      - cat examples/nvcf/ci/ci_create_helm_default.json.template | envsubst > create_helm_file.json
      - cat examples/nvcf/ci/ci_deploy_helm_default.json.template | envsubst > deploy_helm_file.json
      - |
        if [[ "$CI_PIPELINE_SOURCE" == "web" ]]; then
          cat deploy_helm_file.json | jq --arg image_repository "$NVCF_STAGING_IMAGE" '.configuration.image.repository = $image_repository' > deploy_helm_file.json.new
        else
          cat deploy_helm_file.json | jq --arg image_repository "$NVCF_DEV_IMAGE" '.configuration.image.repository = $image_repository' > deploy_helm_file.json.new
        fi
      - mv deploy_helm_file.json.new deploy_helm_file.json
      - |
        echo "NVCF DEPLOYMENT"
        echo "*************************************"
        echo "NVCF_IMAGE:         $NVCF_IMAGE"
        echo "NVCF_BACKEND:       $NVCF_BACKEND"
        echo "NVCF_GPU_TYPE:      $NVCF_GPU_TYPE"
        echo "NVCF_INSTANCE_TYPE: $NVCF_INSTANCE_TYPE"
        echo "AWS_DEFAULT_REGION: $AWS_DEFAULT_REGION"
        echo "HELM_CHART_NAME:    $CICD_HELM_CHART_NAME"
        echo "HELM_CHART_VERSION: $CICD_HELM_CHART_VERSION"
        echo "*************************************"
      # Create NVCF function
      - cosmos-curate nvcf function create-function --name "${HELM_FUNCTION_NAME}" --health-ep /api/local_raylet_healthz --health-port 52365 --helm-chart https://helm.ngc.nvidia.com/${NVCF_ORG_ID}/charts/${CICD_HELM_CHART_NAME}-${CICD_HELM_CHART_VERSION}.tgz --data-file create_helm_file.json
      # Copy funcid.json from ~/.config to working directory
      - cp ~/.config/cosmos_curate/funcid.json funcid_working.json
      # Deploy NVCF function
      - cosmos-curate nvcf function deploy-function --data-file deploy_helm_file.json --max-concurrency $NVCF_MAX_CONCURRENCY --instance-count $NVCF_INSTANCE_COUNT
      - |
        while true; do
          status=$(cosmos-curate nvcf function get-deployment-detail | grep "Status")
          if [[ $status =~ "DEPLOYING" ]]; then
            echo "Waiting for deployment to be active... (retrying in 10 seconds)"
            sleep 10
          elif [[ $status =~ "ACTIVE" ]]; then
            echo "Deployment is active"
            break
          else
            echo "Error: Deployment status '$status' is not recognized."
            exit 1
          fi
        done
      # Invoke NVCF function
      # Create S3 credentials file
      - awk -v ak="$AWS_ACCESS_KEY_ID" -v sk="$AWS_SECRET_ACCESS_KEY" -v rg="$AWS_DEFAULT_REGION"  '/aws_access_key_id/ {$0 = "aws_access_key_id = " ak} /aws_secret_access_key/ {$0 = "aws_secret_access_key = " sk} /region/ {$0 = "region = " rg} {print}' examples/nvcf/creds/aws_credentials > aws_credentials
      - sed -i '/^endpoint/d' aws_credentials
      # Determine whether to raise on pynvc error based on GPU type
      - |
        if [[ "$NVCF_GPU_TYPE" =~ (L40|GB) ]]; then
          export DECODE_RAISE_ON_ERROR=true
        else
          export DECODE_RAISE_ON_ERROR=false
        fi
      # Split-annotate pipeline
      # default config to process 1 video
      - |
        cat examples/nvcf/function/invoke_video_split_full.json | \
        jq --arg output_clip_path "$S3_OUTPUT_CLIP_PATH" --arg input_video_path "$S3_INPUT_VIDEO_PATH" --argjson limit 1 \
        '.args.output_clip_path = $output_clip_path | .args.input_video_path = $input_video_path | .args.limit = $limit' > invoke_split1.json
      - cosmos-curate nvcf function invoke-function --data-file invoke_split1.json --s3-config-file aws_credentials
      # use GPU decoding & FP8 for Qwen to process 1 more video
      - |
        cat examples/nvcf/function/invoke_video_split_full.json | \
        jq --arg output_clip_path "$S3_OUTPUT_CLIP_PATH" --arg input_video_path "$S3_INPUT_VIDEO_PATH" --argjson limit 1 --arg transnetv2_frame_decoder_mode pynvc --argjson transnetv2_frame_decode_raise_on_pynvc_error "$DECODE_RAISE_ON_ERROR" --argjson transcode_use_hwaccel true --argjson qwen_use_fp8_weights true \
        '.args.output_clip_path = $output_clip_path | .args.input_video_path = $input_video_path | .args.limit = $limit | .args.transnetv2_frame_decoder_mode = $transnetv2_frame_decoder_mode | .args.transnetv2_frame_decode_raise_on_pynvc_error = $transnetv2_frame_decode_raise_on_pynvc_error | .args.transcode_use_hwaccel = $transcode_use_hwaccel | .args.qwen_use_fp8_weights = $qwen_use_fp8_weights ' > invoke_split2.json
      - cosmos-curate nvcf function invoke-function --data-file invoke_split2.json --s3-config-file aws_credentials
      # Dedup pipeline
      - cat examples/nvcf/function/invoke_video_dedup.json | jq --arg output_path "$S3_OUTPUT_DEDUP_PATH" --arg input_embeddings_path "$S3_OUTPUT_CLIP_PATH/iv2_embd_parquet/" '.args.output_path = $output_path | .args.input_embeddings_path = $input_embeddings_path' > invoke_dedup.json
      - cosmos-curate nvcf function invoke-function --data-file invoke_dedup.json --s3-config-file aws_credentials
      # Shard-dataset pipeline
      - cat examples/nvcf/function/invoke_video_shard.json | jq --arg output_dataset_path "$S3_OUTPUT_DATASET_PATH" --arg input_clip_path "$S3_OUTPUT_CLIP_PATH" --arg input_semantic_dedup_path "$S3_OUTPUT_DEDUP_PATH" '.args.output_dataset_path = $output_dataset_path | .args.input_clip_path = $input_clip_path | .args.input_semantic_dedup_path = $input_semantic_dedup_path' > invoke_shard.json
      - cosmos-curate nvcf function invoke-function --data-file invoke_shard.json --s3-config-file aws_credentials
  after_script:
    - *activate_micromamba
    - cosmos-curate nvcf config set --org $NVCF_ORG_ID --key $NVCF_KEY
    - |
      if [[ "$HELM_DEBUG_KEEP_FAILED_DEPLOYMENTS" =~ "False" ]]; then
        if [ ! -f ~/.config/cosmos_curate/funcid.json ]; then
          echo "~/.config/cosmos_curate/funcid.json not found, using working directory copy"
          mkdir -p ~/.config/cosmos_curate/
          cp funcid_working.json ~/.config/cosmos_curate/funcid.json
        fi
        cosmos-curate nvcf function delete-function
      else
        echo "Intentionally leaving deployment behind for debugging. This must be manually cleaned up later."
      fi
  artifacts:
    when: always
    paths:
      - "invoke_split1.json"
      - "invoke_split2.json"
      - "invoke_dedup.json"
      - "invoke_shard.json"
      - "trace.log"
      - "/tmp/*.log"

# Run validation tests against S3 artifacts from NVCF function
nvcf-helm-validate:
  stage: end-to-end
  needs: [ "nvcf-helm" ]
  rules: *merge_result_and_web_rules
  allow_failure: false
  image: mambaorg/micromamba:latest
  tags:
    - build-runner  # Override to Docker executor
  variables:
    S3_FILE_SPLIT_SUMMARY: "${S3_OUTPUT_CLIP_PATH}/summary.json"
    S3_FILE_DEDUP_SUMMARY: "${S3_OUTPUT_DEDUP_PATH}/extraction/dedup_summary_0.01.csv"
    S3_FILE_SHARD_SUMMARY: "${S3_OUTPUT_DATASET_PATH}/v0/wdinfo_list.csv"
  before_script:
    - *setup_micromamba
    - pip3 install awscli --upgrade --user
    - export PATH="$HOME/.local/bin:$PATH"
    - hash -r
    - awk -v ak="$AWS_ACCESS_KEY_ID" -v sk="$AWS_SECRET_ACCESS_KEY" -v rg="$AWS_DEFAULT_REGION"  '/aws_access_key_id/ {$0 = "aws_access_key_id = " ak} /aws_secret_access_key/ {$0 = "aws_secret_access_key = " sk} /region/ {$0 = "region = " rg} {print}' examples/nvcf/creds/aws_credentials > aws_credentials
    - sed -i '/^endpoint/d' aws_credentials
    - aws --version
    - export AWS_SHARED_CREDENTIALS_FILE="$(pwd)/aws_credentials"
    - |
      if ! which jq; then
        micromamba install -y -c conda-forge jq
      fi
  script:
    - |
      set +e  # Disable exit on error
      i=0
      max=10
      file_found=false
      while [ $i -lt $max ]; do
          if aws s3 ls "${S3_FILE_SPLIT_SUMMARY}" &> /dev/null; then
              echo "Found S3 split-annotate summary file: ${S3_FILE_SPLIT_SUMMARY}"
              file_found=true
              break
          else
              echo "Waiting for S3 data in ${S3_FILE_SPLIT_SUMMARY} ... ($i of $max, retrying in 5 seconds)"
              sleep 5
              ((i++))
          fi
      done
      set -e
      if [ "$file_found" = false ]; then
          echo "S3 command failed, retry limit exceeded."
          exit 1
      fi
    - |
      echo "Validating $S3_FILE_SPLIT_SUMMARY ..."
      JSON_CONTENT=$(aws s3 cp "$S3_FILE_SPLIT_SUMMARY" - 2>/dev/null)
      if [ $? -ne 0 ]; then
          echo "Error reading from S3: $S3_FILE_SPLIT_SUMMARY"
          exit 1
      fi
      if ! jq empty <<< "$JSON_CONTENT" 2>/dev/null; then
          echo "Error: Invalid JSON structure"
          exit 1
      fi
      num_videos=$(jq -r ".num_processed_videos" <<< "$JSON_CONTENT")
      if [ "$num_videos" -ne 2 ]; then
          echo "Error: There should be 2 videos processed, but found $num_videos"
          exit 1
      fi
      num_clips=$(jq -r ".num_clips_transcoded" <<< "$JSON_CONTENT")
      num_clips_with_caption=$(jq -r ".num_clips_with_caption" <<< "$JSON_CONTENT")
      if [ "$num_clips" -lt 2 ]; then
          echo "Error: There should be at least 2 clips transcoded, but found $num_clips"
          exit 1
      fi
      if [ "$num_clips_with_caption" -ne "$num_clips" ]; then
          echo "Error: All clips should have captions, but found $num_clips_with_caption out of $num_clips"
          exit 1
      fi
      echo "Split-annotate pipeline finished with $num_videos processed and $num_clips_with_caption captioned"
      echo "Split-annotate pipeline validation successful"
    - |
      set +e  # Disable exit on error
      i=0
      max=10
      file_found=false
      while [ $i -lt $max ]; do
          if aws s3 ls "${S3_FILE_DEDUP_SUMMARY}" &> /dev/null; then
              echo "Found S3 dedup summary file: ${S3_FILE_DEDUP_SUMMARY}"
              file_found=true
              break
          else
              echo "Waiting for S3 data in ${S3_FILE_DEDUP_SUMMARY} ... ($i of $max, retrying in 5 seconds)"
              sleep 5
              ((i++))
          fi
      done
      set -e
      if [ "$file_found" = false ]; then
          echo "S3 command failed, retry limit exceeded."
          exit 1
      fi
      echo "Dedup pipeline validation successful"
    - |
      set +e  # Disable exit on error
      i=0
      max=10
      file_found=false
      while [ $i -lt $max ]; do
          if aws s3 ls "${S3_FILE_SHARD_SUMMARY}" &> /dev/null; then
              echo "Found S3 dedup summary file: ${S3_FILE_SHARD_SUMMARY}"
              file_found=true
              break
          else
              echo "Waiting for S3 data in ${S3_FILE_SHARD_SUMMARY} ... ($i of $max, retrying in 5 seconds)"
              sleep 5
              ((i++))
          fi
      done
      set -e
      if [ "$file_found" = false ]; then
          echo "S3 command failed, retry limit exceeded."
          exit 1
      fi
      echo "Shard-dataset pipeline validation successful"

promote_to_staging:
  stage: post-merge
  variables:
    GIT_STRATEGY: none
  rules: *post_merge_rules
  script:
    - IMAGE_TAG="${CI_COMMIT_TIMESTAMP%%T*}_${CI_COMMIT_SHORT_SHA}"
    - echo "Promoting to staging from $IMAGE_TAG"
    - *docker_login
    - DEV="${NVCF_DEV_IMAGE}:${IMAGE_TAG}"
    - STAGING="${NVCF_STAGING_IMAGE}:${IMAGE_TAG}"
    - docker buildx imagetools create --tag "${STAGING}" "${DEV}"

# Promote the latest staging image to prod on a nightly schedule
promote_to_prod:
  stage: post-merge
  rules: *schedule_rules
  script:
    - echo "Promoting latest staging image to prod"
    - *docker_login
    - *setup_micromamba
    - *setup_curator_cli
    - cosmos-curate nvcf config set --org $NVCF_ORG_ID --key $NVCF_KEY
    # Determine staging tag by inspecting annotation on the latest staging image
    - STAGING_TAG=$(cosmos-curate nvcf image list-image-detail --iname $NVCF_STAGING_BASE_IMAGE |grep latestTag |sed "s/['│,]//g" |awk '{print $2}')
    - echo "Extracted staging tag ${STAGING_TAG}"
    - STAGING_IMAGE="${NVCF_STAGING_IMAGE}:${STAGING_TAG}"
    - PROD_IMAGE="${NVCF_PROD_IMAGE}:${STAGING_TAG}"
    - echo "Copying $STAGING_IMAGE -> $PROD_IMAGE"
    - docker buildx imagetools create --tag "$PROD_IMAGE" "$STAGING_IMAGE"

build-packages:
  stage: post-merge
  rules: *schedule_rules
  before_script:
    - *setup_micromamba
    - TIMESTAMP=$(date -d ${CI_COMMIT_TIMESTAMP} +%s)
    - VERSION=$(grep -m 1 'version = ' pyproject.toml | cut -d'"' -f2)
    - PKG_VERSION="${VERSION}.dev${TIMESTAMP}"
    - echo "Building package for ${PKG_VERSION}"
    - pip install twine --disable-pip-version-check
    - poetry version "${PKG_VERSION}"
    - poetry build --no-interaction
  script:
    - twine upload --skip-existing -u gitlab-ci-token -p ${CI_JOB_TOKEN} --repository-url
      "${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/pypi" dist/*
    - twine upload --skip-existing -u ${ARTIFACTORY_USER} -p ${ARTIFACTORY_TOKEN} --repository-url
      "${ARTIFACTORY_URL}" dist/*
